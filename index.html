<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ğŸ“· Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¬ÙˆÙ‡</title>
  <script defer src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
  <style>
    body {
      font-family: 'Cairo', sans-serif;
      text-align: center;
      background-color: #f0f0f0;
      direction: rtl;
      margin: 0;
      padding: 20px;
    }

    h1 {
      color: #006233;
    }

    video, canvas {
      border-radius: 12px;
      max-width: 95vw;
    }

    #info {
      margin-top: 15px;
      font-size: 18px;
      color: green;
    }

    button {
      padding: 10px 20px;
      font-size: 16px;
      margin: 10px;
      border: none;
      border-radius: 10px;
      background-color: #006233;
      color: white;
      cursor: pointer;
    }
  </style>
</head>
<body>
  <h1>ğŸ“· Ù†Ø¸Ø§Ù… Ø§Ù„Ø­Ø¶ÙˆØ± Ø¨Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¬ÙˆÙ‡</h1>
  <button onclick="toggleCamera()">ğŸ” ØªØ¨Ø¯ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§</button>
  <br>
  <video id="video" autoplay muted playsinline width="640" height="480" style="border:2px solid #006233;"></video>
  <canvas id="overlay"></canvas>
  <div id="info">ğŸ“¦ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬...</div>

  <script>
    const video = document.getElementById('video');
    const overlay = document.getElementById('overlay');
    const context = overlay.getContext('2d');
    const info = document.getElementById('info');

    let currentStream;
    let usingFrontCamera = false;
    let faceMatcher;
    let attendanceSet = new Set();

    // Ø§Ø³ØªØ¨Ø¯Ù„ Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø­Ø³Ø¨ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù„Ø¯ students/
    const studentLabels = ['Ø£Ø­Ù…Ø¯', 'ÙØ§Ø·Ù…Ø©', 'adel', 'imad'];

    async function loadModelsAndFaces() {
      await faceapi.nets.tinyFaceDetector.loadFromUri('/models');
      await faceapi.nets.faceLandmark68Net.loadFromUri('/models');
      await faceapi.nets.faceRecognitionNet.loadFromUri('/models');

      const labeledDescriptors = [];

      for (const label of studentLabels) {
        try {
          const img = await faceapi.fetchImage(`/students/${label}.jpg`);
          const detection = await faceapi
            .detectSingleFace(img)
            .withFaceLandmarks()
            .withFaceDescriptor();

          if (detection) {
            labeledDescriptors.push(new faceapi.LabeledFaceDescriptors(label, [detection.descriptor]));
          } else {
            console.warn(`ğŸš« Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ ÙˆØ¬Ù‡ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©: ${label}`);
          }
        } catch (err) {
          console.error(`âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø©: ${label}`, err);
        }
      }

      faceMatcher = new faceapi.FaceMatcher(labeledDescriptors, 0.6);
      info.innerText = 'âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„ÙˆØ¬ÙˆÙ‡ Ø¨Ù†Ø¬Ø§Ø­';
    }

    async function startCamera() {
      if (currentStream) {
        currentStream.getTracks().forEach(track => track.stop());
      }

      const constraints = {
        video: {
          facingMode: usingFrontCamera ? 'user' : 'environment'
        },
        audio: false
      };

      try {
        currentStream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = currentStream;
      } catch (err) {
        console.error('âŒ ÙØ´Ù„ ÙÙŠ ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§:', err);
        info.innerText = 'âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§';
      }
    }

    function toggleCamera() {
      usingFrontCamera = !usingFrontCamera;
      startCamera();
    }

    async function recognizeFaces() {
      video.addEventListener('play', () => {
        overlay.width = video.videoWidth;
        overlay.height = video.videoHeight;

        setInterval(async () => {
          const detections = await faceapi
            .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
            .withFaceLandmarks()
            .withFaceDescriptors();

          context.clearRect(0, 0, overlay.width, overlay.height);

          const resized = faceapi.resizeResults(detections, {
            width: overlay.width,
            height: overlay.height
          });

          for (const detection of resized) {
            const bestMatch = faceMatcher.findBestMatch(detection.descriptor);
            const name = bestMatch.label;

            const box = detection.detection.box;
            context.strokeStyle = '#00ff00';
            context.lineWidth = 3;
            context.strokeRect(box.x, box.y, box.width, box.height);

            context.fillStyle = '#006233';
            context.fillRect(box.x, box.y - 20, box.width, 20);
            context.fillStyle = 'white';
            context.font = '16px Cairo';
            context.fillText(name, box.x + 5, box.y - 5);

            if (name !== 'unknown' && !attendanceSet.has(name)) {
              attendanceSet.add(name);
              info.innerText = `âœ… ØªÙ… ØªØ³Ø¬ÙŠÙ„ Ø­Ø¶ÙˆØ±: ${name}`;
              sendToGoogleSheet(name);
            }
          }
        }, 1500);
      });
    }

    async function sendToGoogleSheet(name) {
      const date = new Date().toLocaleDateString('ar-DZ');
      const time = new Date().toLocaleTimeString('ar-DZ');

      try {
        await fetch('https://script.google.com/macros/s/YOUR_SCRIPT_ID/exec', {
          method: 'POST',
          body: JSON.stringify({ name, date, time }),
        });
      } catch (err) {
        console.error('âŒ ÙØ´Ù„ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Google Sheets:', err);
      }
    }

    // Ø¹Ù†Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©
    window.addEventListener('load', async () => {
      await loadModelsAndFaces();
      await startCamera();
      recognizeFaces();
    });
  </script>
</body>
</html>
